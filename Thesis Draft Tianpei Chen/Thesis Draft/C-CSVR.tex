\resetdatestamp



\chapter{Complex Support Vector Preliminary Detector (CSVPD) for Large-Scale MIMO Uplink Systems}\label{Chapter CSVPD}
 

 


In this chapter, we proposed a complex support vector preliminary detector (CSVPD), which has a complexity  comparable with linear detectors but performs much better. The proposed CSVPD can be widely used as the preliminary processor to generate initial solutions for the detectors whose performances are heavily depended on the initial solution.
 The proposed CSVPD can work in the complex valued Large-Scale MIMO (LS-MIMO) systems in an elegant and efficient manner.  Furthermore, based on the analysis of channel hardening phenomenon from Chapter 2 and the recent advances in machine learning field concerning decomposition process for SVM\cite{steinwart2011training}, we propose a combined single direction searching strategy in CSVR training process which can approximately maximize the gain of the sub dual objective functions whose work set size are two. The combined single direction searching strategy proposed can achieve a much smaller searching times than the optimal double direction searching while make dual objective function converge using as few iterations as the latter one. 
  
  Furthermore, we show that channel hardening phenomenon can be further exploited in CSVPD to reduce the computational complexity and simplify the implementation of the algorithm for LS-MIMO systems.
  
   Finally,  we compare the performance of CSVPD aided likelihood ascend search (LAS) detector (CSVPD-LAS) and CSVPD aided parallel interference cancellation with ordering (OPIC) detector (CSVPD-OPIC) with MMSE-LAS and MMSE-OPIC. The simulation results demonstrate the capability of CSVPD to be utilized as a preliminary detector to generate more reliable initial solutions.  


\section{Brief Introduction to $\epsilon$-Support Vector Regression ($\epsilon$-SVR)}
\subsection{primal Objective Function}\label{DualityForm}
  Suppose we are given a training data $((\mathbf{x}_{1}, y_{1}),(\mathbf{x}_{2},y_{2}),\cdots,(\mathbf{x}_{L},y_{L}))$, $L$ denotes the size of the training data set, $\mathbf{x}_{i}\in \mathbb{R}^{V}$ denotes input data vector, $V$ is the number of features in $\mathbf{x}_{i}$. $y_{i}$ denotes the output. Let $\mathbf{w}$ denotes regression coefficient vector, $\Phi(\mathbf{x}_{i})$ denotes the feature mapping of $\mathbf{x}_{i}$, $\mathbf{w},\Phi(\mathbf{x}_{i})\in \mathbb{R}^{\Omega}$, $\Omega \in \mathbb{R}$ denotes the dimension of mapped feature space (For linear model, $\mathbf{x}_{i}=\Phi(\mathbf{x}_{i})$, $\Omega=V$). The regression estimate $g(\mathbf{x}_{i})$ (either linear or non-linear) is given by 
\begin{equation}
g(\mathbf{x}_{i})=\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b  \quad i= 1,2,\ldots, L
\label{regression estimates}
\end{equation} 
\begin{figure}
\centering
\def\svgwidth{\columnwidth}
\input{regression_model.pdf_tex}
\caption{Regression Model of $\epsilon$-SVR}
\label{regression model}
\end{figure}

 In $\epsilon$-SVR, as shown in Fig \ref{regression model}, the solid line represents the regression estimate in \ref{regression estimates}, $\epsilon$ controls the precision of the regression, the area between two dash lines ($\epsilon$-bound) is called $\epsilon$-tube. Only the regression estimates that have the deviation larger than $\epsilon$ (the data points located outside $\epsilon$-tube in Fig.\ref{regression model}) denoted by $\xi_{i}$ and $\hat{\xi}_{i}$, contribute to the estimation errors.
the goal of $\epsilon$-SVR is to minimize the risk introduced by estimate errors while keeping $\mathbf{w}$ small, which is also called regularized risk minimization principle, therefore, the primal constraint optimization problem is formulated as
\begin{eqnarray}
\nonumber
\min_{\mathbf{w}, \xi, \hat{\xi}_{i}} f(\mathbf{w}, \xi_{i}, \hat{\xi}_{i})=\frac{1}{2}||\mathbf{w}||^{2}+C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))\\
s.t. \left\{\begin{array}{ll}
y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b \leq \epsilon+\xi_{i}, i=1,2\cdots, L \\
\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i} \leq \epsilon+\hat{\xi}_{i}, i=1,2\cdots, L\\
\epsilon, \xi_{i},\hat{\xi}_{i} \geq 0, i=1,2\cdots, L
\end{array}\right.
\label{primal optimization problem}
\end{eqnarray} 
In (\ref{primal optimization problem}), $\frac{1}{2}||\mathbf{w}||^{2}$ is the regularization term in order to ensure the flatness of regression model.
 slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ are introduced based on the "soft margin" principle\cite{cortes1995support} that can cope with the infeasible constraints of the optimization problem and allows the existence of some additive noise to the observations. $R(u)$ denotes the cost function. The simplest cost function is $R(u)=u$, The choice of the cost function is determined by the statistical distribution of the additive noise\cite{smola2004tutorial}. For example if the noise is Gaussian distributed, then the optimal cost function is $R(u)=\frac{1}{2}u^{2}$. The term $C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))$ denotes the penalty of the additive noise, $C\in \mathbb{R}$ and $C\geq 0$ that controls the trade off between regularization term and cost function term.
 
In $\epsilon$-SVR, the objective to exploit slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ is to compensate the influences from the outliers that exceed the $\epsilon$-tube which are caused by noise.


\subsection{Cost Function}\label{cost function}
The performance of SV regression is significantly depends on the choice of the cost functions$R(\xi_{i})+R(\hat{\xi}_{i})$ \cite{smola1998connection}\cite{muller1997predicting}, 
The optimal cost function choice in (\ref{primal optimization problem}) can be determined based on maximum likelihood (ML) principle. Assume the data samples $\mathbf{x}_{i}$ in data set are iid, Let $f_{true}(\mathbf{x}_{i}), i=1,2,\cdots L$ denotes true regression function. the underlying assumption is $y_{i}=f_{true}(\mathbf{x}_{i})+\varepsilon_{i}, i=1,2,\cdots, L$, $\varepsilon_{i}$ denotes additive noise of the $i$th data sample, with probability density function (pdf) $Pr(\cdot)$. Let $P(\cdot)$ denotes the pdf of $y_{i}$. Based on ML principle we want to 
\begin{eqnarray}
\nonumber
\max_{f}\quad \prod_{i=1}^{L}P(y_{i}|f(\mathbf{x}_{i}))=\prod_{i=1}^{L}P(f(\mathbf{x}_{i})+\varepsilon_{i}|f(\mathbf{x}_{i}))=\prod_{i=1}^{L}Pr(\varepsilon_{i})=\prod_{i=1}^{L}Pr(y_{i}-f(\mathbf{x}_{i}))\\
\label{cost function1}
\end{eqnarray}
Take the logarithm of $\prod_{i=1}^{L}Pr(y_{i}-f(\mathbf{x}_{i}))$, (\ref{cost function1}) is equivalent to 
\begin{eqnarray}
\max_{f} \sum_{i=1}^{L}\log(Pr(y_{i}-f(\mathbf{x}_{i}))),
\label{cost function2}
\end{eqnarray}
(\ref{cost function2}) is equivalent to 
\begin{equation}
\min_{f}-\sum_{i=1}^{L}\log(Pr(y_{i}-f(\mathbf{x}_{i}))),
\label{cost function3}
\end{equation}
Therefore, the cost function of $i$th input-out data sample, denoted by  $c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))$ can be defined as 
\begin{equation}
c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))\varpropto -\log(Pr(y_{i}-f(\mathbf{x}_{i}))).
\label{cost function4}
\end{equation}
Thus (\ref{cost function3}) is equivalent to 
\begin{equation}
\min_{f}\quad  \sum_{i=1}^{L}c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i})),
\label{Total risk function}
\end{equation} 
%In $\epsilon$-SVR, in order to provide more flexibility to precision control, Vapnik's $\epsilon$-insensitive function, as shown in (\ref{Vepsilon}), is applied to (\ref{cost function3}).
%\begin{eqnarray}
%|u|_{\epsilon}=\left\{\begin{array}{ll}
%0   &if\quad |u|<\epsilon\\
%|u|-\epsilon  &otherwise\\
%\end{array}\right.
%\label{Vepsilon}
%\end{eqnarray}
In the discrete time model of LS-MIMO systems in (\ref{discrete time MIMO systems}), the additive white noise is Gaussian distributed, thus $-\log(Pr(\varepsilon_{i}))\varpropto \frac{1}{2}\varepsilon^{2}_{i}$, based on (\ref{cost function4}), the final form of cost function in SVD can be written as 
\begin{equation}
c_{LS-MIMO}(\mathbf{x}_{i}, y_{i} f(\mathbf{x}_{i}))= \frac{1}{2}(y_{i}-f(\mathbf{x}_{i}))^{2},
\label{final cost function}
\end{equation}  
In $\epsilon$-SVR, the slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ are used to cope with the outliers of $\epsilon$-tube caused by additive noise, that is  
\begin{eqnarray}
\label{definition of slack variable1}
\xi_{i}=\max(0, y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon)\\
\label{definition of slack variable2}
\hat{\xi}_{i}=\max(0, \mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon).
\end{eqnarray} 
If $\epsilon$ is set to be zero, that (\ref{definition of slack variable1}) and (\ref{definition of slack variable2}) can be rewritten as 
\begin{eqnarray}
\label{definition2 of slack variable1}
\xi_{i}=\max(0, y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b)\\
\label{definition2 of slack variable2}
\hat{\xi}_{i}=\max(0, \mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}).
\end{eqnarray} 
From (\ref{definition2 of slack variable1}) and (\ref{definition2 of slack variable2}), there is at most one of $\xi_{i}$ and $\hat{\xi}_{i}$ can be non zero. That is $\xi_{i}\hat{\xi}_{i}=0$. 
By defining $R(u)=\frac{1}{2}u^{2}$ in (\ref{primal optimization problem}), we have  
\begin{equation}
R(\xi_{i})+R(\hat{\xi}_{i})=\frac{1}{2}\xi^{2}_{i}+\frac{1}{2}\hat{\xi}^{2}_{i}=\frac{1}{2}(y_{i}-f(\mathbf{x}_{i}))^{2}=c_{LS-MIMO}(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))
\label{final form of cost function in LS-MIMO}
\end{equation} 
In conclusion, the cost function in LS-MIMO CSVD is defined as 
\begin{equation}
C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))=\frac{1}{2}(y_{i}-f(\mathbf{x}_{i}))^{2},
\label{LS-MIMO cost function}
\end{equation}
\subsection{Dual Objective Function}\label{dual objective Function}
According to Lagrange Theorem, the constraint optimization problem (\ref{primal optimization problem}) can be transformed to Lagrangian dual form by combining the original optimization function with inequality constraints, the combination coefficient is called Lagrange multiplier. The Lagrange function is given by 
\begin{eqnarray}
\nonumber
L(\mathbf{w}, b, \xi_{i}, \hat{\xi}_{i}, \alpha_{i}, \hat{\alpha}_{i}, \eta_{i}, \hat{\eta}_{i})=
\frac{1}{2}||\mathbf{w}||^{2}+C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))-\sum_{i=1}^{L}(\eta_{i}\xi_{i}+\hat{\eta}_{i}\hat{\xi}_{i})\\
\nonumber
+\sum_{i=1}^{L}\alpha_{i}(y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon-\xi_{i})+\sum_{i=1}^{L}\hat{\alpha}_{i}(\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon-\hat{\xi}_{i})\\
s.t. \left\{\begin{array}{cc}
\eta_{i}, \hat{\eta}_{i}, \alpha_{i}, \hat{\alpha}_{i}\geq 0, i=1,2,\cdots L\\
\xi_{i}, \hat{\xi}_{i}\geq  0, i=1,2,\cdots L\\
\end{array}\right.
\label{lagrange duality1}
\end{eqnarray}
where $\eta_{i}$, $\hat{\eta}_{i}$, $\alpha_{i}$, $\hat{\alpha}_{i}$ are Lagrange multipliers.

The sufficient and necessary conditions such that a solution $\mathbf{w}$ of the primal constrained optimization problem in (\ref{primal optimization problem}) satisfies, are called Karush-Kuhn-Tucker (KKT) conditions. Here we elaborate a little further about how the dual objective problem is derived from KKT conditions.

Assume a constraint optimization problem is given by 
\begin{eqnarray}
\nonumber
\min_{\mathbf{w}} \quad f(\mathbf{w})\\
s.t. \quad c_{i}(\mathbf{w})\leq 0, i=1,2,\ldots L,
\label{constraint optimization problem}
\end{eqnarray}
its Lagrange function is given by 
\begin{equation}
L(\mathbf{w}, \mathbf{a})=f(\mathbf{w})+\sum_{i=1}^{L}a_{i}c_{i}(\mathbf{w}),
\label{orginal Lagrange}
\end{equation}
where $\mathbf{a}=[a_{1},a_{2},\ldots, a_{L}]^{T}$ denotes the vector consist of Lagrange multipliers.
 Based on Theorem 6.21 in \cite{scholkopf2002learning}, a variable pair $[\bar{\mathbf{w}}, \bar{\mathbf{a}}]$ is the solution to (\ref{constraint optimization problem}) if and only if the following inequalities are satisfied
 \begin{equation}
 L(\mathbf{w}, \bar{\mathbf{a}})\geq L(\bar{\mathbf{w}}, \bar{\mathbf{a}})\geq L(\bar{\mathbf{w}}, \mathbf{a})
 \label{KKT inequalities}
 \end{equation}
These inequalities (also called saddle point conditions) holds if and only if KKT conditions holds (see Theorem 6.26 \cite{scholkopf2002learning}), which are 
\begin{eqnarray}
\label{original KKT condition1}
\partial_{\mathbf{w}}L(\bar{\mathbf{w}}, \mathbf{a})=\partial_{\mathbf{w}}f(\bar{\mathbf{w}})+\sum_{i=1}^{L}a_{i}\partial_{\mathbf{w}}c_{i}(\bar{\mathbf{w}})=0,\\
\label{original KKT condition2}
\partial_{a_{i}}L(\bar{\mathbf{w}}, \bar{\mathbf{a}})=c_{i}(\bar{\mathbf{w}})\leq 0, i=1,2,\ldots L\\
\label{original KKT condition3}
\bar{a}_{i}c_{i}(\bar{\mathbf{w}})=0, i=1,2,\ldots L
\end{eqnarray}
Therefore, KKT conditions are the necessary and sufficient conditions for optimality.
In order to satisfy the first inequality in (\ref{KKT inequalities}), (\ref{original KKT condition1}) has to hold,
applying (\ref{original KKT condition1}) to (\ref{lagrange duality1}), which are  
\begin{eqnarray}
\label{partial1}
\partial_{\mathbf{w}} L=\mathbf{w}-\sum_{i=1}^{l}(\alpha_{i}-\hat{\alpha}_{i})\Phi(\mathbf{x}_{i})=0\\
\label{partial2}
\partial_{\xi_{i}}L=C_{i}R^{'}(\xi_{i})-\eta_{i}-\alpha_{i}=0, i=1,2,\cdots L\\
\label{parial3}
\partial_{\hat{\xi}_{i}}L=C_{i}R^{'}(\hat{\xi_{i}})-\hat{\eta}_{i}-\hat{\alpha}_{i}=0, i=1,2,\cdots L\\
\label{partial4}
\partial_{b}L=\sum_{i=1}^{l}(\alpha_{i}-\hat{\alpha}_{i})=0\\\nonumber
\end{eqnarray}
Then by substituting (\ref{partial1})-(\ref{partial4}) to (\ref{lagrange duality1}), (\ref{lagrange duality1}) can be rewritten as :
\begin{eqnarray}
\nonumber
\theta(\alpha_{i}, \hat{\alpha}_{i})=\frac{1}{2}\sum_{i=1}^{L}\sum_{j=1}^{L}(\alpha_{i}-\hat{\alpha_{i}})(\alpha_{j}-\hat{\alpha_{j}})\Phi^{T}(\mathbf{x}_{j})\Phi(\mathbf{x}_{i})+C\sum_{i=1}^{L}[R(\xi_{i})+R(\hat{\xi}_{i})]-\sum_{i=1}^{L}[(CR^{'}(\xi_{i})-\alpha_{i})\xi_{i}\\
\nonumber
+(CR^{'}(\hat{\xi}_{i})-\hat{\alpha}_{i})\hat{\xi}_{i}]+\sum_{i=1}^{L}\alpha_{i}[y_{i}-\sum_{j=1}^{L}(\alpha_{j}-\hat{\alpha}_{j})\Phi^{T}(\mathbf{x}_{j})\Phi(\mathbf{x}_{i})-b-\epsilon-\xi_{i}]+\\
\sum_{i=1}^{L}\hat{\alpha}_{i}[\sum_{j=1}^{L}(\alpha_{j}-\hat{\alpha}_{j})\Phi^{T}(\mathbf{x}_{j})\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon-\hat{\xi}_{i}]
\label{lagrange duality1 inter}
\end{eqnarray}
notice in (\ref{partial4}), $\sum_{i=1}^{L}(\alpha_{i}-\hat{\alpha}_{i})=0$, define $\tilde{R}(u)=R(u)-uR^{'}(u)$, (\ref{lagrange duality1 inter}) can be further simplified to
\begin{eqnarray}
\nonumber
\theta(\alpha_{i}, \hat{\alpha}_{i}) =-\frac{1}{2}\sum_{i=1}^{L}\sum_{i=1}^{L}(\alpha_{i}-\hat{\alpha_{i}})(\alpha_{j}-\hat{\alpha_{j}})\Phi(\mathbf{x}_{j})^{T}\Phi(\mathbf{x}_{i})+C\sum_{i=1}^{L}[(\tilde{R}(\xi_{i})+\tilde{R}(\hat{\xi}_{i})]\\
+\sum_{i=1}^{L}[(\alpha_{i}-\hat{\alpha_{i}})y_{i}-(\alpha_{i}+\hat{\alpha_{i}})\epsilon]
\label{dual objective function1}
\end{eqnarray}
In order to satisfy the second inequality in (\ref{KKT inequalities}), (\ref{original KKT condition2}) and  (\ref{original KKT condition3}) have to hold. A $\mathbf{w}$ that satisfies the conditions in (\ref{original KKT condition2}) is in the feasible region defined by inequality constraints as mentioned (\ref{constraint optimization problem}). In $\epsilon$-SVR, this condition is satisfied by making use of the slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ defined in (\ref{definition of slack variable1}) and (\ref{definition of slack variable2}). Define that $\tilde{\mathbf{w}}$ is in the feasible region and satisfies the conditions in (\ref{original KKT condition1}), notice $\theta(\alpha_{i}, \hat{\alpha}_{i})$ is equivalent to $L(\tilde{\mathbf{w}}, \mathbf{a})$, thus yielding the dual optimization problem, which is given by 
\begin{eqnarray}
\nonumber
\max_{\mathbf{a}}L(\tilde{\mathbf{w}},\mathbf{a})\equiv\max_{\alpha_{i}, \hat{\alpha}_{i}}\quad \theta(\alpha_{i}, \hat{\alpha}_{i}) =-\frac{1}{2}\sum_{i=1}^{L}\sum_{j=1}^{L}(\alpha_{i}-\hat{\alpha_{i}})(\alpha_{j}-\hat{\alpha_{j}})\Phi^{T}(\mathbf{x}_{j})\Phi(\mathbf{x}_{i})+C\sum_{i=1}^{L}(\tilde{R}(\xi_{i})+\tilde{R}(\hat{\xi}_{i}))\\
+\sum_{i=i}^{L}[(\alpha_{i}-\hat{\alpha_{i}})y_{i}-(\alpha_{i}+\hat{\alpha_{i}})\epsilon]
\label{dual optimization function2}
\end{eqnarray}
Define $\mathbf{a}=[\alpha_{1},\alpha_{2}, \ldots, \alpha_{L}]^{T} $, $\mathbf{\hat{a}}=[\hat{\alpha}_{1},\hat{\alpha}_{2}, \ldots, \hat{\alpha}_{L}]^{T} $, $\mathbf{y}=[y_{1}, y_{2}, \ldots y_{L}]^{T}$, $ \mathbf{e}=[1,1,\ldots,1]^{T}\in \mathbb{R}^{L}$, $\mathbf{e}_{i}$ denotes the vector that only $i$th component is 1 while the rest are all 0, $\mathbf{R}_{\xi}=[\tilde{R}(\xi_{1}), \tilde{R}(\xi_{2}), \ldots, \tilde{R}(\xi_{L})]^{T}$, $\mathbf{R}_{\hat{\xi}}=[\tilde{R}(\hat{\xi}_{1}), \tilde{R}(\hat{\xi}_{2}), \ldots, \tilde{R}(\hat{\xi}_{L})]^{T}$, $\mathbf{K}_{ij}=\Phi(\mathbf{x}_{j})^{T}\Phi(\mathbf{x}_{i})$ denotes a component of data kernel matrix at $i$th row and $j$th column. An alternative vector form of (\ref{dual optimization function2}) can be written as 
\begin{equation}
\max_{\mathbf{a}, \hat{\mathbf{a}}}\theta(\mathbf{a},\hat{\mathbf{a}})=-\frac{1}{2}(\mathbf{a}-\mathbf{\hat{a}})^{T}\mathbf{K}(\mathbf{a}-\mathbf{\hat{a}})+(\mathbf{y}-\epsilon \mathbf{e})^{T}\mathbf{a}+(-\mathbf{y}-\epsilon\mathbf{e})^{T}\mathbf{\hat{a}}+C\mathbf{e}^{T}(\mathbf{R}_{\xi}+\mathbf{R}_{\hat{\xi}}),
\label{dual objective alternative}
\end{equation} 
We define the following $2L$ vectors 
$\mathbf{a}^{(*)}$= [$\mathbf{a} \atop \mathbf{\hat{a}}$], $\mathbf{v}\in \mathbb{R}^{2L}$, 
\begin{eqnarray}
[\mathbf{v}]_{i}=\left\{\begin{array}{ll}
1\quad i=1,\ldots, l\\
-1\quad i=l+1,\ldots, 2l\\ 
\end{array}\right.
\end{eqnarray}
(\ref{dual objective alternative}) can also be reformulate as 
%\newcommand{\mysmallarraydecl}{\renewcommand{%
%\IEEEeqnarraymathstyle}{\scriptscriptstyle}%
%\renewcommand{\IEEEeqnarraytextstyle}{\scriptsize}%
%\renewcommand{\baselinestretch}{1.1}%
%\settowidth{\normalbaselineskip}{\scriptsize
%\hspace{\baselinestretch\baselineskip}}%
%\setlength{\baselineskip}{\normalbaselineskip}%
%\setlength{\jot}{0.25\normalbaselineskip}%
%\setlength{\arraycolsep}{2pt}}
%\begin{equation}
%\max_{\mathbf{a}^{*}}\quad \theta(\mathbf{a}^{*})=-\frac{1}{2}(\mathbf{a}^{*})^{T}\left[\begin{IEEEeqnarraybox*}[\mysmallarraydecl][c]{,c/c,}
%\mathbf{K}& -\mathbf{K}\\
%-\mathbf{K}& \mathbf{K}
%\end{IEEEeqnarraybox*}\right]
%\mathbf{a}^{(*)}+[(\mathbf{y}-\epsilon)^{T}, (-\mathbf{y}-\epsilon)^{T}]\mathbf{a}^{(*)}+C\mathbf{e}^{T}(\mathbf{R}_{\xi}+\mathbf{R}_{\hat{\xi}}),
%\label{vectorize dual optimization function}
%\end{equation} 

Condition in (\ref{original KKT condition3}) is called KKT complementary condition, the value of $\sum_{i=1}^{L}\bar{a}_{i}c_{i}(\bar{\mathbf{w}})$ can be used to monitor the proximity of the current solution and the optimal solution. Thus it can be used as a stopping criterion. In the constraint optimization problem of (\ref{lagrange duality1}), the KKT complementary conditions are given by 
\begin{eqnarray}
\left\{\begin{array}{cc}
\alpha_{i}(y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon-\xi_{i})=0, i=1,2\cdots L\\
\hat{\alpha}_{i}(\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon-\hat{\xi}_{i})=0, i=1,2\cdots L\\
(CR^{'}(\xi_{i})-\alpha_{i})\xi_{i}=0, i=1,2,\ldots, L\\
(CR^{'}(\hat{\xi}_{i})-\hat{\alpha}_{i})\hat{\xi}_{i}=0, i=1,2,\ldots, L\\
\end{array}\right. 
\label{KKT complimentary}
\end{eqnarray}
Based on the definitions of slack variables in (\ref{definition of slack variable1}) an (\ref{definition of slack variable2}), only when there is a outlier exists, $\xi_{i}$ or $\hat{\xi}_{i}$ can be non zero, that is 
\begin{equation}
\xi_{i} or \hat{\xi}_{i}=|\mathbf{y}_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})|_{\epsilon},
\label{definition of slack variable3}
\end{equation} 
because the distance of the estimation $\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b$ and the observation $y_{i}$ can only exceeds the $\epsilon$-tube in one direction, as shown in Fig. \ref{epsilon-SVR}. Therefore at most one of $(y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon-\xi_{i})$  and $(\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon-\hat{\xi}_{i})$ can be zero. In order to satisfy the KKT complementary conditions in (\ref{KKT complimentary}), at least one of $\alpha_{i}$ and $\hat{\alpha}_{i}$ need to be zero, that is $\alpha_{i}\hat{\alpha}_{i}=0$. 

Therefore the complete KKT complementary conditions of $\epsilon$-SVR is given by
\begin{eqnarray}
\left\{\begin{array}{cc}
\alpha_{i}(y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon-\xi_{i})=0, i=1,2\cdots L\\
\hat{\alpha}_{i}(\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon-\hat{\xi}_{i})=0, i=1,2\cdots L\\
(CR^{'}(\xi_{i})-\alpha_{i})\xi_{i}=0, i=1,2,\ldots, L\\
(CR^{'}(\hat{\xi}_{i})-\hat{\alpha}_{i})\hat{\xi}_{i}=0, i=1,2,\ldots, L\\
\xi_{i}\hat{\xi}_{i}=0, \alpha_{i}\hat{\alpha}_{i}=0, i=1,2,\ldots L
\end{array}\right. 
\label{complete KKT complimentary}
\end{eqnarray}

\section{Combined Single Direction Searching Strategy}
Decomposition methods were proposed to solve this QP problem by decomposing it into sub QP problems and solving them iteratively\cite{platt1999fast}. Therefore, the computational intensive numerical methods can be avoided. Decomposition is performed by sub set selection solver, which refers to a set of algorithms that separate the optimization variables (Lagrange multipliers) into two sets S and N, S is the work set and N contains the remaining optimization variables. In each iteration, only the optimization variables in the work set is updated while keeping other optimization variables fixed. The Sequential Minimal Optimization (SMO) algorithm\cite{platt1999fast} is an extreme case of the decomposition solvers. An important issue of the sub set selection solvers is the selection of the work set. One strategy is to choose Karush-Kuhn-Tucker (KKT) condition violators, ensuring the final converge\cite{osuna1997improved}. The SMO algorithm restricts the size of the work set to two. A method to train SVM without offset was proposed In\cite{steinwart2011training}, with the comparable performance to the SVM with offset. A set of sequential single variable work set selection strategies, which require $O(n)$ searching time are proposed. The optimal double variable work set selection strategy, which performs exhaustively searching, however, requires $O(n^{2})$ searching time. The authors demonstrate that with the combination of two different proposed single variable work set selection strategies, convergence can be achieved by a iteration time that is as few as optimal double variable work set selection strategy.

The mathematical foundation of kernel based methods is RKHS which is defined in complex domain, however most of the practitioners are dealing with real data sets. In communication and signal processing area, the channel gains, signals, waveforms etc. are all represented in complex form. Recently, a pure complex SVR \& SVM based on complex kernel was proposed in\cite{bouboulis2013complex}, which can deal with the complex data set purely in complex domain. The results in\cite{bouboulis2013complex} demonstrate the better performance as well as reduced complexity comparing to simply split learning task into two real case by real kernels.  
Based on this work, we derive a complexity-performance controllable detector for large MIMO systems based on a dual channel complex SVR (CSVR). The detector can work in two parallel real SVR channels which can be solved independently. Moreover, only the real part of kernel matrix is needed in both channels. This means a large amount of computation cost saving can be achieved.
Based on the discrete time MIMO channel model, in our regression model, this CSVR-detector
is constructed without offset, Therefore, for each real SVR without offset, 
%in principle, only one variable is needed to be updated in each iteration, In our scheme, a sequential single variable selection strategy is proposed. By this strategy, two variables can be updated at each iteration, with much smaller searching time.
Two types of combined single optimization variable selection strategy are proposed based on the work in \cite{steinwart2011training}. The proposed combined single optimization variable selection strategy can approximate optimal double optimization variable selection strategy. The former one can achieve as few as iteration time while enjoy significant speed gain in each iteration.
\section{Complex Support Vector Preliminary Detector}
\section{Channel Hardening Approximation}
\section{CSVPD-LAS versus MMSE-LAS}
The single solution based metaheuristics which are also defined as trajectory methods, start from an initial solution and the search process performs on a single solution at any time as a trajectory in the searching space. In resent years, several single solution based metaheuristic algorithms are applied to LS-MIMO detection problem, such as likelihood ascend searching (LAS) algorithm and variants\cite{vardhan2008low}\cite{cerato2009hardware}\cite{li2010multiple} and Tabu search algorithms and variants\cite{srinidhi2011layered}\cite{datta2010random}. 
\section{CSVPD-OPIC versus MMSE-OPIC}
