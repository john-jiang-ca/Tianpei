\resetdatestamp



\chapter{Complex Support Vector Preliminary Detector (CSVPD) for Large-Scale MIMO Uplink Systems}
The key challenge in designing Large-Scale MIMO (LS-MIMO) detectors is to reduce complexities while maintaining high performances. Given the redundant diversity gain potential provided by LS-MIMO\cite{oestges2010mimo} that can only be achieved at very high Signal to Noise Ratio (SNR), one may consider to sacrifice the redundant diversity gains in order to reduce complexities. Furthermore, the ML criterion of MIMO detection stated in (\ref{MLD}) indicates MIMO detection can be naturally classified as a Combinatorial Optimization (CO) problem, that consist of the search for the optimal solution in a discrete and finite set. 

Based on the aforementioned situations, the metaheuristics algorithmic frameworks are of intense research interest in LS-MIMO detection in recent years. Metaheuristics refer to the high level algorithmic strategies that guide subordinate heuristics iteratively by combining different intelligent learning operations. Metaheuristics can explore and exploit the searching space efficiently and find near optimal solutions without costing high complexities. The class of algorithms includes but not restrict to Iterative Local Search (ILS), Tabu Search (TS), Simulated Annealing (SA), Genetic Algorithms (GA) and Ant Colony Optimization (ACO).

The single solution based metaheuristics which are also defined as trajectory methods, start from an initial solution and the search process performs on a single solution at any time as a trajectory in the searching space. In resent years, several single solution based metaheuristics algorithms are applied to LS-MIMO detection problem, such as 

 One of the common characteristics shared by single solution based metaheuristics algorithms is their performances are influenced by initial solution, the performances are improved when starting from a promising region of searching space  

 


In this chapter, we proposed a complex support vector preliminary detector (CSVPD), which has a complexity  comparable with linear detectors but performs much better. The proposed CSVPD can work in the complex valued Large-Scale MIMO (LS-MIMO) systems in an elegant and efficient manner. We exploit the specific widely linear regression model without constant offset, which is suitable for LS-MIMO systems model. Furthermore, based on the analysis of channel hardening phenomenon from Chapter 2 and the recent advances in machine learning field concerning decomposition process for SVM\cite{steinwart2011training}, we propose a combined single direction searching strategy in CSVR training process which can approximately maximize the gain of the sub dual objective functions whose work set size are two. The combined single direction searching strategy proposed can achieve a much smaller searching times than the optimal double direction searching while make dual objective function converge using as few iterations as the latter one. 
  
  Furthermore, we show that channel hardening phenomenon can be further exploited in CSVPD to reduce the computational complexity and simplify the implementation of the algorithm for LS-MIMO systems. Finally, we compare the performance of CSVPD aided likelihood ascend search (LAS) detector and CSVPD aided parallel interference cancellation with ordering (OPIC) detector with MMSE-LAS and MMSE-OPIC. These results demonstrate the potential of CSVPD to be utilized as a preliminary detector that can improve the LS-MIMO detectors whose performances are influenced by the reliability of initial solutions.  


\section{Brief Introduction to $\epsilon$-Support Vector Regression ($\epsilon$-SVR)}
\subsection{primal Objective Function}\label{DualityForm}
  Suppose we are given a training data $((\mathbf{x}_{1}, y_{1}),(\mathbf{x}_{2},y_{2}),\cdots,(\mathbf{x}_{L},y_{L}))$, $L$ denotes the size of the training data set, $\mathbf{x}_{i}\in \mathbb{R}^{V}$ denotes input data vector, $V$ is the number of features in $\mathbf{x}_{i}$. $y_{i}$ denotes the output. Let $\mathbf{w}$ denotes regression coefficient vector, $\Phi(\mathbf{x}_{i})$ denotes the feature mapping of $\mathbf{x}_{i}$, $\mathbf{w},\Phi(\mathbf{x}_{i})\in \mathbb{R}^{\Omega}$, $\Omega \in \mathbb{R}$ denotes the dimension of mapped feature space (For linear model, $\mathbf{x}_{i}=\Phi(\mathbf{x}_{i})$, $\Omega=V$). The regression estimate $g(\mathbf{x}_{i})$ (either linear or non-linear) is given by 
\begin{equation}
g(\mathbf{x}_{i})=\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b  \quad i= 1,2,\ldots, L
\label{regression estimates}
\end{equation} 
\begin{figure}
\centering
\def\svgwidth{\columnwidth}
\input{regression_model.pdf_tex}
\caption{Regression Model of $\epsilon$-SVR}
\label{regression model}
\end{figure}

 In $\epsilon$-SVR, as shown in Fig \ref{regression model}, the solid line represents the regression estimate in \ref{regression estimates}, $\epsilon$ controls the precision of the regression, the area between two dash lines ($\epsilon$-bound) is called $\epsilon$-tube. Only the regression estimates that have the deviation larger than $\epsilon$ (the data points located outside $\epsilon$-tube in Fig.\ref{regression model}) denoted by $\xi_{i}$ and $\hat{\xi}_{i}$, contribute to the estimation errors.
the goal of $\epsilon$-SVR is to minimize the risk introduced by estimate errors while keeping $\mathbf{w}$ small, which is also called regularized risk minimization principle, therefore, the primal constraint optimization problem is formulated as
\begin{eqnarray}
\nonumber
\min_{\mathbf{w}, \xi, \hat{\xi}_{i}} f(\mathbf{w}, \xi_{i}, \hat{\xi}_{i})=\frac{1}{2}||\mathbf{w}||^{2}+C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))\\
s.t. \left\{\begin{array}{ll}
y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b \leq \epsilon+\xi_{i}, i=1,2\cdots, L \\
\mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i} \leq \epsilon+\hat{\xi}_{i}, i=1,2\cdots, L\\
\epsilon, \xi_{i},\hat{\xi}_{i} \geq 0, i=1,2\cdots, L
\end{array}\right.
\label{primal optimization problem}
\end{eqnarray} 
In (\ref{primal optimization problem}), $\frac{1}{2}||\mathbf{w}||^{2}$ is the regularization term in order to ensure the flatness of regression model.
 slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ are introduced based on the "soft margin" principle\cite{cortes1995support} that can cope with the infeasible constraints of the optimization problem and allows the existence of some additive noise to the observations. $R(u)$ denotes the cost function. The simplest cost function is $R(u)=u$, The choice of the cost function is determined by the statistical distribution of the additive noise\cite{smola2004tutorial}. For example if the noise is Gaussian distributed, then the optimal cost function is $R(u)=\frac{1}{2}u^{2}$. The term $C\sum_{i=1}^{L}(R(\xi_{i})+R(\hat{\xi}_{i}))$ denotes the penalty of the additive noise, $C\in \mathbb{R}$ and $C\geq 0$ that controls the trade off between regularization term and cost function term.
 
In $\epsilon$-SVR, the objective to exploit slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ is to compensate the influences from the outliers that exceed the $\epsilon$-tube which are caused by noise.


\subsection{Cost Function}\label{cost function}
The optimal cost function ($R(\xi_{i})+R(\hat{\xi}_{i})$) in (\ref{primal optimization problem}) can be derived based on maximum likelihood (ML) principle. Assume the data samples $\mathbf{x}_{i}$ in data set are iid, Let $f_{true}(\mathbf{x}_{i}), i=1,2,\cdots L$ denotes true regression function. the underlying assumption is $y_{i}=f_{true}(\mathbf{x}_{i})+\varepsilon_{i}, i=1,2,\cdots, L$, $\varepsilon_{i}$ denotes additive noise of the $i$th data sample, with probability density function (pdf) $Pr(\cdot)$. Let $P(\cdot)$ denotes the pdf of $y_{i}$. Based on ML principle we want to 
\begin{eqnarray}
\nonumber
\max_{f}\quad \prod_{i=1}^{L}P(y_{i}|f(\mathbf{x}_{i}))=\prod_{i=1}^{L}P(f(\mathbf{x}_{i})+\varepsilon_{i}|f(\mathbf{x}_{i}))=\prod_{i=1}^{L}Pr(\varepsilon_{i})=\prod_{i=1}^{L}Pr(y_{i}-f(\mathbf{x}_{i}))\\
\label{cost function1}
\end{eqnarray}
Take the logarithm of $\prod_{i=1}^{L}Pr(y_{i}-f(\mathbf{x}_{i}))$, (\ref{cost function1}) is equivalent to 
\begin{eqnarray}
\max_{f} \sum_{i=1}^{L}\log(Pr(y_{i}-f(\mathbf{x}_{i}))),
\label{cost function2}
\end{eqnarray}
maximizing (\ref{cost function2}) is equivalent to minimizing $-\sum_{i=1}^{L}\log(Pr(y_{i}-f(\mathbf{x}_{i})))$, therefore (\ref{cost function1}) is equivalent to 
\begin{equation}
\min_{f}-\sum_{i=1}^{L}\log(Pr(y_{i}-f(\mathbf{x}_{i}))),
\label{cost function3}
\end{equation}
Let $c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))$ denotes the cost function of $i$th input-out data sample, which is defined as  
\begin{equation}
c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))\varpropto -\log(P(y_{i}-f(\mathbf{x}_{i}))).
\label{cost function4}
\end{equation}
Thus (\ref{cost function3}) is equivalent to 
\begin{equation}
\min_{f}\quad  \sum_{i=1}^{L}c(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i})),
\label{Total risk function}
\end{equation} 
In $\epsilon$-SVR, in order to provide more flexibility to precision control, Vapnik's $\epsilon$-insensitive function, as shown in (\ref{Vepsilon}), is applied to (\ref{cost function3}).
\begin{eqnarray}
|u|_{\epsilon}=\left\{\begin{array}{ll}
0   &if\quad |u|<\epsilon\\
|u|-\epsilon  &otherwise\\
\end{array}\right.
\label{Vepsilon}
\end{eqnarray}
Thus the final form of cost function in $\epsilon$-SVR can be written as 
\begin{equation}
\tilde{c}(y_{i}, \mathbf{x}_{i}, f(\mathbf{x}_{i}))\varpropto -\log(Pr(|y_{i}-f(\mathbf{x}_{i})|_{\epsilon})),
\label{final cost function}
\end{equation}  
the cost function term is determined according to the distribution of noise. In discrete time model of LS-MIMO systems, noise is AWGN, that is $-\log(Pr(\varepsilon_{i}))\varpropto \frac{1}{2}\varepsilon^{2}_{i}$, based on (\ref{final cost function}), the cost function in LS-MIMO SVD defined as 
\begin{equation}
c_{LS-MIMO}(\mathbf{x}_{i}, y_{i}, f(\mathbf{x}_{i}))=\frac{(|y_{i}-f(\mathbf{x}_{i})|^{2}_{\epsilon})}{2}, 
\label{cost function in LS-MIMO}
\end{equation} 
In $\epsilon$-SVR, the slack variables $\xi_{i}$ and $\hat{\xi}_{i}$ are used to cope with the outliers of $\epsilon$-tube caused by additive noise, that is  
\begin{eqnarray}
\label{definition of slack variable1}
\xi_{i}=\max(0, y_{i}-\mathbf{w}^{T}\Phi(\mathbf{x}_{i})-b-\epsilon)\\
\label{definition of slack variable2}
\hat{\xi}_{i}=\max(0, \mathbf{w}^{T}\Phi(\mathbf{x}_{i})+b-y_{i}-\epsilon).
\end{eqnarray} 
Because the deviation between the estimates $g(\mathbf{x}_{i})$ and the observation $y_{i}$ in \ref{regression estimates} can only exceed the $\epsilon$-tube in one direction, therefore there is at most one of $\xi_{i}$ and $\hat{\xi}_{i}$ can be non zero. That is $\xi_{i}\hat{\xi}_{i}=0$. 

In conclusion, the cost function term in (\ref{primal optimization problem}) can be defined as $R(u)=\frac{1}{2}u^{2}$, thus we have 
\begin{equation}
R(\xi_{i})+R(\hat{\xi}_{i})=\frac{1}{2}\xi^{2}_{i}+\frac{1}{2}\hat{\xi}^{2}_{i}=\frac{1}{2}|y_{i}-f(\mathbf{x}_{i})|^{2}_{\epsilon}
\label{final form of cost function in LS-MIMO}
\end{equation} 
\subsection{Dual Objective Problem}

\section{Combined Single Direction Searching Strategy}
Decomposition methods were proposed to solve this QP problem by decomposing it into sub QP problems and solving them iteratively\cite{platt1999fast}. Therefore, the computational intensive numerical methods can be avoided. Decomposition is performed by sub set selection solver, which refers to a set of algorithms that separate the optimization variables (Lagrange multipliers) into two sets S and N, S is the work set and N contains the remaining optimization variables. In each iteration, only the optimization variables in the work set is updated while keeping other optimization variables fixed. The Sequential Minimal Optimization (SMO) algorithm\cite{platt1999fast} is an extreme case of the decomposition solvers. An important issue of the sub set selection solvers is the selection of the work set. One strategy is to choose Karush-Kuhn-Tucker (KKT) condition violators, ensuring the final converge\cite{osuna1997improved}. The SMO algorithm restricts the size of the work set to two. A method to train SVM without offset was proposed In\cite{steinwart2011training}, with the comparable performance to the SVM with offset. A set of sequential single variable work set selection strategies, which require $O(n)$ searching time are proposed. The optimal double variable work set selection strategy, which performs exhaustively searching, however, requires $O(n^{2})$ searching time. The authors demonstrate that with the combination of two different proposed single variable work set selection strategies, convergence can be achieved by a iteration time that is as few as optimal double variable work set selection strategy.

The mathematical foundation of kernel based methods is RKHS which is defined in complex domain, however most of the practitioners are dealing with real data sets. In communication and signal processing area, the channel gains, signals, waveforms etc. are all represented in complex form. Recently, a pure complex SVR \& SVM based on complex kernel was proposed in\cite{bouboulis2013complex}, which can deal with the complex data set purely in complex domain. The results in\cite{bouboulis2013complex} demonstrate the better performance as well as reduced complexity comparing to simply split learning task into two real case by real kernels.  
Based on this work, we derive a complexity-performance controllable detector for large MIMO systems based on a dual channel complex SVR (CSVR). The detector can work in two parallel real SVR channels which can be solved independently. Moreover, only the real part of kernel matrix is needed in both channels. This means a large amount of computation cost saving can be achieved.
Based on the discrete time MIMO channel model, in our regression model, this CSVR-detector
is constructed without offset, Therefore, for each real SVR without offset, 
%in principle, only one variable is needed to be updated in each iteration, In our scheme, a sequential single variable selection strategy is proposed. By this strategy, two variables can be updated at each iteration, with much smaller searching time.
Two types of combined single optimization variable selection strategy are proposed based on the work in \cite{steinwart2011training}. The proposed combined single optimization variable selection strategy can approximate optimal double optimization variable selection strategy. The former one can achieve as few as iteration time while enjoy significant speed gain in each iteration.
\section{Complex Support Vector Preliminary Detector}
\section{Channel Hardening Approximation}
\section{CSVPD-LAS versus MMSE-LAS}
\section{CSVPD-OPIC versus MMSE-OPIC}
